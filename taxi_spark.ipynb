{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6750ee55-e554-41cf-92ce-61acfb2a6831",
   "metadata": {},
   "source": [
    "## TLC Trip Record Data\n",
    "The yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "The For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possibvle, complete and accurate information.\n",
    "\n",
    "## Data source\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "## Method\n",
    "- Using the pipline , import CSV files using pyarrow: \n",
    "- PySpark SQL to merge files, transform data, and analyse the Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e88b5c-30f0-4ea6-a4d3-9c43e0e1077d",
   "metadata": {},
   "source": [
    "## Download csv files and transform them to paquet files\n",
    "Using the pipeline explained in the Readme file. you can download csv files and transform them to paquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f258fb-5ad2-4ec7-bb88-4053d1bd158f",
   "metadata": {},
   "source": [
    "## What we do here\n",
    "Here first initialize the spark. Please install pyspark first. Then we\n",
    "1. Read all parquet files for green and yellow taxi and check schema\n",
    "2. Create two extra columns for *'hourOfDay'* and *'dayofweek'*\n",
    "3. Merge all files in parquet and avra format\n",
    "4. Apply some queries to check the data\n",
    "\n",
    "Then we merge all parquet files, transform files to avra, transform data, and analyse the Data\n",
    "For the initial run, we would like to have a new file.  After the runing the code, we refuse to replace the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934ccb6-e25c-4346-ac18-2acc7454fded",
   "metadata": {},
   "source": [
    "## Starting the spark to calculate some observable and query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7aa2f5a-02db-407a-8b0a-05de688f7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 11:19:11 WARN Utils: Your hostname, LAPTOP-5GKK0U9L resolves to a loopback address: 127.0.1.1; using 172.29.201.166 instead (on interface eth0)\n",
      "22/04/11 11:19:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alireza/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/11 11:19:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/11 11:19:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.startTime', '1649668762646'), ('spark.memory.offHeap.size', '8g'), ('spark.sql.warehouse.dir', 'file:/home/alireza/taxi-statistic/spark-warehouse'), ('spark.driver.host', '172.29.201.166'), ('spark.executor.id', 'driver'), ('spark.driver.port', '39441'), ('spark.app.name', 'taxi-Data'), ('spark.rdd.compress', 'True'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.memory.offHeap.enabled', 'True'), ('spark.app.id', 'local-1649668765436'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "<pyspark.sql.session.SparkSession object at 0x7fc33ce4a100>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "     .builder\\\n",
    "     .master(\"local[*]\")\\\n",
    "     .config(\"spark.executor.memory\", \"8g\")\\\n",
    "     .config(\"spark.driver.memory\", \"8g\")\\\n",
    "     .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "     .config(\"spark.memory.offHeap.size\",\"8g\")   \\\n",
    "     .appName(\"taxi-Data\")\\\n",
    "     .getOrCreate()\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e872ef-30b8-4e63-b727-a6a3cccabc25",
   "metadata": {},
   "source": [
    "### 1. Read all parquet files for green and yellow taxi and check schema\n",
    "First we read all files in the *'file_path'* directory created by pipeline and check their schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7878201-08bf-4763-a941-71cccb45ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green taxi information:\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|       2| 2018-10-01 02:05:48|  2018-10-01 02:21:49|                 N|         1|         255|          97|              2|         4.37|       16.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        17.3|           2|        1|\n",
      "|       2| 2018-10-01 02:24:19|  2018-10-01 02:31:50|                 N|         1|          97|          49|              2|         1.45|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|           2|        1|\n",
      "|       2| 2018-10-01 02:12:06|  2018-10-01 02:21:15|                 N|         1|          25|         181|              1|         2.04|        9.0|  0.5|    0.5|       1.5|         0.0|     null|                  0.3|        11.8|           1|        1|\n",
      "|       2| 2018-10-01 02:34:42|  2018-10-01 02:39:23|                 N|         1|          25|          40|              1|         0.91|        5.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.3|           2|        1|\n",
      "|       2| 2018-10-01 02:50:21|  2018-10-01 03:01:28|                 N|         1|          25|         257|              1|         3.53|       12.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        13.8|           2|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Yellow taxi information:\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2019-01-01 01:46:40|  2019-01-01 01:53:20|              1|          1.5|         1|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                null|\n",
      "|       1| 2019-01-01 01:59:47|  2019-01-01 02:18:59|              1|          2.6|         1|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                null|\n",
      "|       2| 2018-12-21 14:48:30|  2018-12-21 14:52:40|              3|          0.0|         1|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|\n",
      "|       2| 2018-11-28 16:52:25|  2018-11-28 16:55:45|              5|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                null|\n",
      "|       2| 2018-11-28 16:56:57|  2018-11-28 16:58:33|              5|          0.0|         2|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read files for green taxi\n",
    "file_path=\"./data/green_par/*\"\n",
    "parqDF_green=spark.read.parquet(file_path)\n",
    "\n",
    "#print schema and show first five columns\n",
    "print('Green taxi information:')\n",
    "parqDF_green.printSchema()\n",
    "parqDF_green.show(n=5)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#Read files for yellow taxi\n",
    "file_path=\"./data/yellow_par/*2019*\"\n",
    "parqDF_yellow=spark.read.parquet(file_path)\n",
    "\n",
    "#print schema and show first five columns\n",
    "print('Yellow taxi information:')\n",
    "parqDF_yellow.printSchema()\n",
    "parqDF_yellow.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c495e-0224-462b-9fad-e2caa1fdec11",
   "metadata": {},
   "source": [
    "### 2. Create two extra columns for *'hourOfDay'* and *'dayofweek'*\n",
    "create to extra columns and add them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64ec50b-6c14-4817-b1cd-31d29f622048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two extra columns for green taxi. and remove 'ehail_fee' for type problem. \n",
    "parqDF_green=parqDF_green.drop('ehail_fee')\n",
    "from pyspark.sql.functions import hour,dayofweek\n",
    "parqDF_green=parqDF_green.withColumn(\"hourOfDay\", hour('lpep_pickup_datetime')).withColumn( \"dayofweek\",dayofweek('lpep_pickup_datetime'))\n",
    "\n",
    "\n",
    "#create two extra columns for yellow taxi\n",
    "parqDF_yellow=parqDF_yellow.withColumn(\"hourOfDay\", hour('tpep_pickup_datetime')).withColumn( \"dayofweek\",dayofweek('tpep_pickup_datetime'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be692-355b-4f04-be9e-b5fff665ee9e",
   "metadata": {},
   "source": [
    "### 3. Merge all files in parquet and avra format\n",
    "Now we can merge them into parquet and avra format. \n",
    "\n",
    "Spark has no intrinsic library to use avra. You should add an external library to spark. \n",
    "\n",
    "To load the library, before starting the spark, type the following command in the terminal\n",
    "\n",
    "```terminal\n",
    "spark-submit --packages org.apache.spark:spark-avro_2.12:3.2.1 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcd670a-a1a2-428b-837d-08100ca1f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.avro\n",
    "\n",
    "#Write parquet for green taxi\n",
    "file_parquet=\"./data/green_merged_parquet\"\n",
    "parqDF_green.write.mode(\"overwrite\").parquet(file_parquet) \n",
    "\n",
    "#Write avro for green taxi\n",
    "file_avro=\"./data/green_merged_avro\"\n",
    "parqDF_green.write.mode(\"overwrite\").save(file_avro)\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "#Write parquet for yellow taxi\n",
    "file_parquet=\"./data/yellow_merged_parquet\"\n",
    "parqDF_yellow.write.mode(\"overwrite\").parquet(file_parquet) \n",
    "\n",
    "#Write avra for yellow taxi\n",
    "file_avro=\"./data/yellow_merged_avro\"\n",
    "parqDF_yellow.write.mode(\"overwrite\").save(file_avro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63a37c-5e59-45db-82d4-8072dfb129d3",
   "metadata": {},
   "source": [
    "### 4. Apply some queries to check the data\n",
    "At the moment, our download data contains 36 month of data between 2018-08 and 2021-07. The data set did not updated ofter that time. \n",
    "\n",
    "First we define two different queries *'ParquetTable_green'* and *'ParquetTable_yellow'* for the tamplate of SQL.\n",
    "It containes the follwoing columns:\n",
    "- *lpep_dropoff_datetime* : drop off date time\n",
    "- *lpep_pickup_datetime*: pick up date time\n",
    "- *trip_distance*: trip distance\n",
    "- *dayofweek*: day of week\n",
    "- *hourOfDay*: number of passangers\n",
    "- *taxi*: kind of taxi *'g'* for green taxi and *'y'* for yellow taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0b40f9-e645-48bc-9941-db9ec2546738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "|lpep_dropoff_datetime|lpep_pickup_datetime|rideTimesec|trip_distance|dayofweek|hourOfDay|passenger_count|taxi|\n",
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "|  2018-10-01 02:21:49| 2018-10-01 02:05:48|        961|         4.37|        2|        2|              2|   g|\n",
      "|  2018-10-01 02:31:50| 2018-10-01 02:24:19|        451|         1.45|        2|        2|              2|   g|\n",
      "|  2018-10-01 02:21:15| 2018-10-01 02:12:06|        549|         2.04|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:39:23| 2018-10-01 02:34:42|        281|         0.91|        2|        2|              1|   g|\n",
      "|  2018-10-01 03:01:28| 2018-10-01 02:50:21|        667|         3.53|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:24:09| 2018-10-01 02:23:15|         54|         0.19|        2|        2|              2|   g|\n",
      "|  2018-10-01 02:28:58| 2018-10-01 02:25:20|        218|         0.83|        2|        2|              1|   g|\n",
      "|  2018-10-01 03:01:02| 2018-10-01 02:40:31|       1231|         4.42|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:40:36| 2018-10-01 02:26:35|        841|          5.9|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:15:06| 2018-10-01 02:13:02|        124|         0.66|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:51:34| 2018-10-01 02:43:37|        477|          2.0|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:25:28| 2018-10-01 02:16:19|        549|         2.29|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:43:42| 2018-10-01 02:29:53|        829|         4.15|        2|        2|              5|   g|\n",
      "|  2018-10-01 02:53:08| 2018-10-01 02:36:31|        997|         9.41|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:24:17| 2018-10-01 02:14:22|        595|         2.04|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:41:51| 2018-10-01 02:20:24|       1287|          4.3|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:09:39| 2018-10-01 02:07:46|        113|          0.5|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:46:22| 2018-10-01 02:43:22|        180|         0.72|        2|        2|              1|   g|\n",
      "|  2018-10-01 02:38:28| 2018-10-01 02:31:24|        424|         1.45|        2|        2|              2|   g|\n",
      "|  2018-10-01 02:17:59| 2018-10-01 02:17:52|          7|          0.0|        2|        2|              1|   g|\n",
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "|lpep_dropoff_datetime|lpep_pickup_datetime|rideTimesec|trip_distance|dayofweek|hourOfDay|passenger_count|taxi|\n",
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "|  2019-01-01 01:53:20| 2019-01-01 01:46:40|        400|          1.5|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:18:59| 2019-01-01 01:59:47|       1152|          2.6|        3|        1|              1|   y|\n",
      "|  2018-12-21 14:52:40| 2018-12-21 14:48:30|        250|          0.0|        6|       14|              3|   y|\n",
      "|  2018-11-28 16:55:45| 2018-11-28 16:52:25|        200|          0.0|        4|       16|              5|   y|\n",
      "|  2018-11-28 16:58:33| 2018-11-28 16:56:57|         96|          0.0|        4|       16|              5|   y|\n",
      "|  2018-11-28 17:28:26| 2018-11-28 17:25:49|        157|          0.0|        4|       17|              5|   y|\n",
      "|  2018-11-28 17:33:43| 2018-11-28 17:29:37|        246|          0.0|        4|       17|              5|   y|\n",
      "|  2019-01-01 01:28:37| 2019-01-01 01:21:28|        429|          1.3|        3|        1|              1|   y|\n",
      "|  2019-01-01 01:45:39| 2019-01-01 01:32:01|        818|          3.7|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:09:32| 2019-01-01 01:57:32|        720|          2.1|        3|        1|              2|   y|\n",
      "|  2019-01-01 01:47:06| 2019-01-01 01:24:04|       1382|          2.8|        3|        1|              2|   y|\n",
      "|  2019-01-01 01:28:24| 2019-01-01 01:21:59|        385|          0.7|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:31:05| 2019-01-01 01:45:21|       2744|          8.7|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:07:42| 2019-01-01 01:43:19|       1463|          6.3|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:15:18| 2019-01-01 01:58:24|       1014|          2.7|        3|        1|              1|   y|\n",
      "|  2019-01-01 01:25:40| 2019-01-01 01:23:14|        146|         0.38|        3|        1|              1|   y|\n",
      "|  2019-01-01 01:48:02| 2019-01-01 01:39:51|        491|         0.55|        3|        1|              1|   y|\n",
      "|  2019-01-01 01:49:07| 2019-01-01 01:46:00|        187|          0.3|        3|        1|              1|   y|\n",
      "|  2019-01-01 02:03:51| 2019-01-01 01:57:45|        366|         1.42|        3|        1|              1|   y|\n",
      "|  2019-01-01 01:25:57| 2019-01-01 01:16:16|        581|         1.72|        3|        1|              1|   y|\n",
      "+---------------------+--------------------+-----------+-------------+---------+---------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parqDF_green.createOrReplaceTempView(\"ParquetTable_green\")\n",
    "parqDF_yellow.createOrReplaceTempView(\"ParquetTable_yellow\")\n",
    "\n",
    "spark.sql('''    SELECT  lpep_dropoff_datetime,\n",
    "                         lpep_pickup_datetime,\n",
    "                         (bigint(lpep_dropoff_datetime)) - (bigint(lpep_pickup_datetime)) AS rideTimesec, \n",
    "                         trip_distance,\n",
    "                         dayofweek, \n",
    "                         hourOfDay,\n",
    "                         passenger_count, \n",
    "                         'g' AS taxi\n",
    "                 FROM ParquetTable_green\n",
    "                    '''\n",
    "          ).show()\n",
    "spark.sql('''    SELECT tpep_dropoff_datetime  AS lpep_dropoff_datetime,\n",
    "                       tpep_pickup_datetime  AS lpep_pickup_datetime,\n",
    "                       (bigint(tpep_dropoff_datetime)) - (bigint(tpep_pickup_datetime)) AS rideTimesec, \n",
    "                       trip_distance,\n",
    "                       dayofweek, \n",
    "                       hourOfDay,\n",
    "                       passenger_count, \n",
    "                      'y' AS taxi\n",
    "                FROM ParquetTable_yellow'''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c68ef-0c31-4b85-bcb6-ab4631481ef8",
   "metadata": {},
   "source": [
    "### Query for The average distance driven by yellow and green taxis per hour\n",
    "The average distance driven by green taxi is 27.79 km/h and by yellow taxi is 9.98 km/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ac17d3-1da7-4543-a47a-849223f7e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (34 + 1) / 35]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------+\n",
      "|taxi|average_distancePerHour|\n",
      "+----+-----------------------+\n",
      "|   g|      27.79075306314514|\n",
      "|   y|      9.988950865483991|\n",
      "+----+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query1='''WITH time_distance AS (\n",
    "                SELECT (bigint(to_timestamp(lpep_dropoff_datetime))) - (bigint(to_timestamp(lpep_pickup_datetime))) AS rideTimesec, \n",
    "                       trip_distance,\n",
    "                       'g' AS taxi\n",
    "                FROM ParquetTable_green\n",
    "                UNION ALL \n",
    "                SELECT (bigint(to_timestamp(tpep_dropoff_datetime))) - (bigint(to_timestamp(tpep_pickup_datetime))) AS rideTimesec, \n",
    "                       trip_distance,\n",
    "                      'y' AS taxi\n",
    "                FROM ParquetTable_yellow\n",
    "                    ) \n",
    "            SELECT taxi,(sum(trip_distance)/ sum(rideTimesec)*3600) AS average_distancePerHour  \n",
    "            from  time_distance\n",
    "            GROUP BY taxi\n",
    "    '''\n",
    "spark.sql( query1 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c6168-15c9-4bdd-86c4-0cb92ddf5ffd",
   "metadata": {},
   "source": [
    "### Query for Day of the week in 2019 and 2020 which has the lowest number of single rider trips\n",
    "Day 1 (Monday) has the lowest number of single rider trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ff428c-592d-431d-b5c4-572cb1e5d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===================================================>    (32 + 3) / 35]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+\n",
      "|dayofweek|count_dayofweek|passenger_count|\n",
      "+---------+---------------+---------------+\n",
      "|        1|        7991247|              1|\n",
      "|        2|        8502436|              1|\n",
      "|        7|        8867558|              1|\n",
      "|        3|        9574237|              1|\n",
      "|        4|        9890865|              1|\n",
      "|        6|       10007853|              1|\n",
      "|        5|       10167470|              1|\n",
      "+---------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query2='''WITH time_distance AS (\n",
    "                SELECT  lpep_pickup_datetime,\n",
    "                       dayofweek,\n",
    "                       passenger_count,\n",
    "                       'g' AS taxi\n",
    "                FROM ParquetTable_green\n",
    "                UNION ALL \n",
    "                SELECT  tpep_dropoff_datetime as lpep_pickup_datetime,\n",
    "                       dayofweek,\n",
    "                       passenger_count,\n",
    "                       'y' AS taxi\n",
    "                FROM ParquetTable_yellow\n",
    "                    ) \n",
    "        SELECT  dayofweek,count(dayofweek) AS count_dayofweek\n",
    "        FROM time_distance\n",
    "        WHERE ((lpep_pickup_datetime BETWEEN CAST('2019-01-01 00:00:01' as Timestamp) AND CAST('2021-01-01 00:00:01' as Timestamp)) ) AND passenger_count=1\n",
    "        GROUP BY dayofweek,passenger_count\n",
    "        ORDER BY count_dayofweek \n",
    "        '''\n",
    "spark.sql( query2 ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f31e8-11a6-4690-977e-7fb3bb5ebd8b",
   "metadata": {},
   "source": [
    "### The top 3 of the busiest hours\n",
    "The top 3 of the busiest hours are 21, 19, and 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c358de6-2dc1-499c-80e4-cdc2c196b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================================>   (33 + 2) / 35]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|hourOfDay|count_hour|\n",
      "+---------+----------+\n",
      "|       20|   6139115|\n",
      "|       19|   6036296|\n",
      "|       21|   5661044|\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query3='''WITH time_distance AS (\n",
    "                SELECT  hourOfDay,\n",
    "                       'g' AS taxi\n",
    "                FROM ParquetTable_green\n",
    "                UNION ALL \n",
    "                SELECT hourOfDay,\n",
    "                      'y' AS taxi\n",
    "                FROM ParquetTable_yellow\n",
    "                    ) \n",
    "            \n",
    "    SELECT  hourOfDay,count(hourOfDay) AS count_hour\n",
    "    FROM time_distance\n",
    "    GROUP BY hourOfDay\n",
    "    ORDER BY count_hour DESC\n",
    "    LIMIT 3\n",
    "    \n",
    "    ;'''\n",
    "spark.sql( query3 ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd9aac-9a8a-4a29-91a4-c1f664cc0645",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "**Q)** Your data scientists want to make future predictions based on weather conditions. How would you expand your pipeline to help your colleagues with this task?\n",
    "\n",
    "**A:** The weather information can be found some other data bases like https://www.visualcrossing.com/weather/weather-data-services which is a payed database.\n",
    "We can a function in the pipeline to add some extra columns like temprature and wheather condition ('sunny', 'rainy', ...) based on the above database.\n",
    "\n",
    "**Q)** Another colleague approaches to you. He is an Excel guru and makes all kind of stuff using this tool forever. So he needs all the available taxi trip records in the XLSX format. Can you re-use your current pipeline? How does this output compares to your existing formats? Do you have performance concerns?\n",
    "\n",
    "**A)** I recommend him to use https://www.cdata.com/drivers/parquet/excel/#:~:text=The%20Parquet%20Excel%20Add%2DIn,based%20data%20analysis%2C%20and%20more! . This Excel Add-In for Parquet Read, Write, and Update Parquet from Excel. The Parquet Excel Add-In is a powerful tool that allows you to connect with live Parquet data, directly from Microsoft Excel. \n",
    "For huge data set XLSX format need huge amound of hard drive. Excel efficency for huge number of columns and rows are so low. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
